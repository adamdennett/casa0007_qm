---
title: "Prof D's Applied Linear Regression Bootcamp"
subtitle: "'WHY?! Why Us? Did we wrong him in a past life?'"
author: 
  - name: "Adam Dennett"
email: "a.dennett@ucl.ac.uk"
date-as-string: "1st August 2024"
other: "CASA0007 Quantitative Methods"
from: markdown+emoji
format:
  revealjs: 
    logo: "images/CASA_logo.svg"
    template-partials: 
      - title-slide.html
    transition: none
    slide-number: TRUE
    preview-links: auto
    theme: casa-slides.scss
    
filters:
 - code-visibility
lightbox: auto
title-slide-attributes:
    data-background-image: "images/title-slide.png"
    data-background-size: stretch
    data-background-opacity: "0.08"
    data-background-color: "#4e3c56"
---

```{r}
#| label: load-data
#| message: false
#| warning: false
#| include: false

#casaviz::view_palette(casaviz::casa_palettes$default, n_colours = 10)

##before we do anything else, let's load the packages we need and the data we require

library(casaviz)
library(tidyverse)
library(sf)
library(plotly)
library(leaflet)
library(rgl)
library(dplyr)
library(here)
library(stringr)
library(dplyr)
library(purrr)
library(janitor)
library(readxl)
library(tibble)
library(ggrepel)
library(gganimate)
#library(easystats)

#all england schools
edubase_schools <- read_csv("https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1") %>% 
  clean_names() %>% 
  mutate(urn = as.character(urn))

#all england schools
#edubase_schools <- read.csv("C:/Users/Adam/Dropbox/Public/edubasealldata20241003.csv") %>% 
#  clean_names() %>% 
#  mutate(urn = as.character(urn))

england_abs <- read_csv(here("wk6", "data", "Performancetables_130242", "2022-2023", "england_abs.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>%
  mutate(URN = as.character(URN))
england_census <- read_csv(here("wk6", "data", "Performancetables_130242", "2022-2023", "england_census.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(5:23, ~ parse_number(as.character(.))))
england_ks4_mats_performance <- read_csv(
  here("wk6", "data", "Performancetables_130242", "2022-2023", "england_ks4-mats-performance.csv"),
  na = c("", "NA", "SUPPMAT", "NP", "NE")
) %>%
  mutate(
    TRUST_UID = as.character(TRUST_UID),
    P8_BANDING = as.character(P8_BANDING),
    INSTITUTIONS_INMAT = as.character(INSTITUTIONS_INMAT)
  ) %>%
  mutate(across(
    .cols = names(.)[11:ncol(.)][!names(.)[11:ncol(.)] %in% c("P8_BANDING", "INSTITUTIONS_INMAT")],
    .fns = ~ parse_number(as.character(.))
  ))

england_ks4_pupdest <- read_csv(here("wk6", "data", "Performancetables_130242", "2022-2023", "england_ks4-pupdest.csv"), na = c("", "NA", "SUPP", "NP", "NE", "SP", "SN")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(8:82, ~ parse_number(as.character(.))))

england_ks4final <- read_csv(here("wk6", "data", "Performancetables_130242", "2022-2023", "england_ks4final.csv"), na = c("", "NA", "SUPP", "NP", "NE", "SP", "LOWCOV", "NEW")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(TOTPUPS:PTOTENT_E_COVID_IMPACTED_PTQ_EE, ~ parse_number(as.character(.))))

england_school_information <- read_csv(
  here("wk6", "data", "Performancetables_130242", "2022-2023", "england_school_information.csv"),
  na = c("", "NA", "SUPP", "NP", "NE", "SP"),
  col_types = cols(
    URN = col_character(),
    OFSTEDLASTINSP = col_date(format = "%d-%m-%Y")  # Adjust format if needed
  )
)


abs_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "abs_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
census_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "census_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4_mats_performance_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "ks4-mats-performance_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4_pupdest_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "ks4-pupdest_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4final_meta <- read_xlsx(here("wk6","data", "Performancetables_130249", "2022-2023", "ks4_meta.xlsx"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
school_information_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "la_and_region_codes_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
la_and_region_codes_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "la_and_region_codes_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
school_information_meta <- read_csv(here("wk6","data", "Performancetables_130249", "2022-2023", "school_information_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()

# str(england_abs)
# str(england_census)
# str(england_ks4_mats_performance)
# str(england_ks4_pupdest)
# str(england_ks4final)
# str(england_school_information)
# str(abs_meta)
# str(census_meta)
# str(ks4_mats_performance_meta)
# str(ks4_pupdest_meta)
# str(ks4final_meta)
# str(school_information_meta)
# str(la_and_region_codes_meta)


```

```{r}

# Custom join function to drop duplicate columns (except URN)
# safe_left_join <- function(x, y) {
#   common_cols <- intersect(names(x), names(y))
#   common_cols <- setdiff(common_cols, "URN")  # keep URN
#   y_clean <- y |> select(-all_of(common_cols))
#   left_join(x, y_clean, by = "URN")
# }

# Perform sequential joins
# england_school_2022_23 <- safe_left_join(england_ks4final) |>
#   safe_left_join(england_abs) |>
#   safe_left_join(england_census) |>
#   safe_left_join(england_ks4_pupdest) |>
#   safe_left_join(england_school_information)

# Left join england_ks4final with england_abs
england_school_2022_23 <- england_ks4final %>%
  left_join(england_abs, by = "URN") %>%
  left_join(england_census, by = "URN") %>%
  left_join(england_school_information, by = "URN")

# Filter out special schools and those with ADMPOL (admissions policy) = "NSE" (non-selective)

england_school_2022_23 <- england_school_2022_23 |>
  left_join(edubase_schools, by = c("URN" = "urn"))

england_school_2022_23_not_special <- england_school_2022_23 %>%
  filter(MINORGROUP != "Special school")

#column_headers_df <- tibble(column_name = names(england_school_2022_23))
  
```

## This week's Session {transition="convex-in none-out" transition-speed="fast"}

```{r, echo=FALSE}
# This makes the fonts play nicely within the figures
knitr::opts_chunk$set(dev = "ragg_png")

```

```{css}
/* This sits here, because it allows us to use images from within the images/ folder. Otherwise, the file structure gets a lot more involved! */
.reveal::after {
  background-image: url('images/light-background.png');
}
```

"This much content in one session?!? Are you mad?":

-   Possibly - But I've marked too many dissertations and examined too many PhDs where people still get the basics wrong and this should help fix that!
-   I've been using this method for 20 years and I **still** get things wrong and I'm *still* learning new things about it!
-   Once you get the basics, it is perhaps *THE* most useful tool in your statistical toolbox
-   Forget about machine learning - regression is your statistical Swiss Army knife!

## This week's Session {transition="convex-in none-out" transition-speed="fast"}

-   Getting the basics right and how this depends on:
    -   understanding the features of your data and your model - sample size, the variables and groups within your data, multicolinearity, heteroscedascity, kurtosis, variance, degrees of freedom, confounding, interaction effects (and what all of these words mean!)
    -   your model is :poop: without a sound theoretical understanding!

## This week's Session {transition="convex-in none-out" transition-speed="fast"}

-   a fool-proof recipe for any future modelling exercise you undertake, including:
    -   the fundamental importance of visualising your data and the relationships you are trying to fit a model to
    -   how to interpret the outputs from your model and how they relate back to the visual patterns you observe and the rules that govern the model
-   an example of how poor urban policy can be based a misunderstanding of data, models and local context, but how the same models, if used correctly, can also underpin better policy

# Our Case Study - What are the Factors Affecting School-Level Educational Attainment in Brighton and Hove? {background-image="https://adamdennett.github.io/BH_Schools_Consultation/attainment_extra_files/figure-html/unnamed-chunk-10-1.png" background="#2e6260" background-opacity="0.1"}

## Secondary Schools and Attainment - GCSEs {transition="convex-in none-out" transition-speed="fast"}

-   Secondary Schools mainly teach children between the ages of 11-16 in England and Wales (some 11-18, some 13-18)
-   The examinations most children take at the end of Year 11 (age 16) are called GCSEs (General Certificate of Secondary Education)
-   The GCSEs are graded from 9 (highest) to 1 (lowest), with a grade of 4 considered a "standard pass" and a grade of 5 considered a "strong pass"

## Secondary Schools and Attainment - Attainment 8 {transition="convex-in none-out" transition-speed="fast"}

-   Attainment 8 is a measure that sums the grades for each pupil across 8 GCSEs (the standard number taken).
    -   Maths is always counted twice and English often counted twice where both language and literature are taken.
    -   Thus a maximum Attainment 8 score of 90 can be achieved
    -   40 = Standard Pass, 50 = Strong Pass
-   The Attainment 8 scores for all year 11 students can be averaged for each school giving a school-level average Attainment 8 Score
-   Attainment 8 is a raw score and doesn't account for important variations in the cohorts of students each school admits or the types of school, so direct comparison between schools without accounting for these factors is risky

## Secondary Schools and Attainment - Progress 8 {transition="convex-in none-out" transition-speed="fast"}

-   Progress 8 is an alternative attainment score which looks at the progress a student makes between arriving at a school in year 7 or 9 and leaving at age 16
-   It compares their levels of attainment at entry and exit with the progress made by similar students nationally
-   Progress 8 is a 'value-added' ratio. A score of zero means students, on average, made expected progress, while a positive score means they made more progress than expected, and a negative score means they made less

## Secondary Schools, Attainment and Urban Policy {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   School performance and pupil attainment can be a big urban policy issue - particularly where variations in access and perceived quality occur
-   These variations feed into broader socio-economic issues in cities
-   In the UK, schools are the responsibility of local government
-   Understanding the drivers behind pupil attainment and school performance vital for effective resource allocation and good local policy
:::

::: {.column width="40%"}
![](images/BHCC_Engagement_PPT.png)
:::
:::::

::: footer
:::

## Secondary Schools, Attainment and Urban Policy {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   In 2024, Brighton and Hove Council convinced that the biggest driver of pupil attainment within schools was concentration of disadvantage
-   ***Disadvantage Attainment Gap*** - socio-economically disadvantaged students perform worse than their more affluent peers
-   Work of Professor Stephen Gorard, University of Durham, suggests mixing of disadvantage improves attainment
-   **Solution**: create more socially mixed schools through a new controversial admissions policy
:::

::: {.column width="40%"}
![](images/gorard_conversation.png)
:::
:::::

::: footer
The Conversation: <https://theconversation.com/poorer-pupils-do-worse-at-school-heres-how-to-reduce-the-attainment-gap-205535>
:::

## Reserarch Question(s) {transition="convex-in none-out" transition-speed="fast"}

-   What are the factors that affect school-level educational attainment in Brighton and Hove?
-   To what extent is attainment driven by social mixing?
-   Are there any other factors that are relevant?
-   What are the implications of this for local policy?
-   Can regression help us and, if it can, how can we go about carefully building a regression model to help us answer these questions?

# A Reliable Regression Modelling Recipe {background-image="https://adamdennett.github.io/BH_Schools_Consultation/attainment_extra_files/figure-html/unnamed-chunk-10-1.png" background="#2e6260" background-opacity="0.1"}

## Recipe / Framework {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   One way to think about regression modelling is as a recipe.
-   The recipe has a number of steps, each of which is important to the final outcome
-   As with baking a cake, failure will follow if you don't:
    -   get your ingredients right
    -   follow the method correctly
    -   use your equipment in the right way
-   You may need to iterate around these a few times during the process
:::

::: {.column width="40%"}
![](https://gu-witness.s3.amazonaws.com/media/31ab0d58-3f7a-44d0-842e-adfcf80f1d8c-mediumoriginalaspectdouble.jpg)
:::
:::::

::: footer
:::

## Step 1 - Ingredients {transition="convex-in none-out" transition-speed="fast"}

-   The Exploratory Data Analysis (EDA) phase is the most important part of any modelling exercise (see Weeks 1 & 2 of this course)
-   It is where you get to know your data (ingredients) - the variables, data types, distributions, any spatial or temporal patterns
-   Failure to get to know your data properly means you might mis-specify your model by:
    -   using the wrong explanatory variables or omitting some key ones
    -   misunderstanding your data types - e.g. counts vs continuous
    -   misunderstanding the relationships between your variables (linear vs logarithmic)
    -   not accounting for important spatial or temporal patterns (autocorrelation) that might mean your observations are not independent

## Step 1a - Ingredients (gathering and preparation) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   The data is collected by the Department for Education (DfE), published annually and is comprehensive - a full annual census of each school
-   For each school, hundreds of variables are available relating to:
    -   attainment and progress
    -   pupil characteristics
    -   school characteristics
-   Some data / variables (ingredients) will be more useful than others
:::

::: {.column width="40%"}
![](images/gov_school_data.png)
:::
:::::

::: footer
DfE Data: <https://www.compare-school-performance.service.gov.uk/compare-schools>
:::

## Step 1a - Ingredients (gathering and preparation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(reactable)
library(casaviz)  # assuming casa_reactable_theme is defined here

england_school_2022_23 %>% 
  filter(LANAME == "Brighton and Hove") %>% 
  filter(phase_of_education_name == "Secondary") %>% 
  filter(establishment_status_name == "Open") %>% 
  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A) |>
  reactable(theme = casa_reactable_theme(colour = "purple"))
```

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(dplyr)

filtered_df <- england_school_2022_23 %>%
  filter(LANAME == "Brighton and Hove",
         phase_of_education_name == "Secondary",
         establishment_status_name == "Open") %>%
  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A, easting, northing)

library(sf)

# Convert to sf object with EPSG:27700
sf_df <- st_as_sf(filtered_df, coords = c("easting", "northing"), crs = 27700)

# Transform to EPSG:4326
sf_df <- st_transform(sf_df, crs = 4326)

# Extract lat/lon for leaflet
sf_df <- sf_df %>%
  mutate(
    lon = st_coordinates(.)[,1],
    lat = st_coordinates(.)[,2]
  )

library(leaflet)
library(scales)

# Define size and color scales
size_scale <- rescale(sf_df$TOTPUPS, to = c(4, 12))  # radius from 4 to 12
# Create custom color palette
casa_palette <- casa_palettes$default
color_scale <- colorNumeric(palette = casa_palette, domain = sf_df$TOTPUPS)


leaflet(sf_df) |>
  addProviderTiles("CartoDB.Positron") |>  # plain, minimal basemap
  addCircleMarkers(
    lng = ~lon,
    lat = ~lat,
    radius = size_scale,
    color = ~color_scale(TOTPUPS),
    stroke = FALSE,
    fillOpacity = 0.8,
    popup = ~paste0(
      "<strong>", SCHNAME.x, "</strong><br>",
      "Pupils: ", TOTPUPS
    )
  ) |> 
  addLegend(
    "bottomright",
    pal = color_scale,
    values = ~TOTPUPS,
    title = "Total Pupils",
    opacity = 0.7
  )
```

-   Visualisation is perhaps the most important part of the EDA phase
-   Always map and graph your data so that you can spot potential issues before they ruin your model!

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

median_value <- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_school_2022_23$ATT8SCR, na.rm = TRUE)
sd_value   <- sd(england_school_2022_23$ATT8SCR, na.rm = TRUE)

ggplot(england_school_2022_23, aes(x = ATT8SCR)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, color = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, color = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, color = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, color = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Attainment 8 - All Schools England and Wales, 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = "Density"
  ) +
  theme_minimal()


```

-   Hmmm? Are there any problems that could be indicated here?

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-boxplot
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(casaviz)  # ensure the package is loaded so its scale functions are available

median_value <- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)

ggplot(england_school_2022_23, aes(x = ATT8SCR, y = "")) +
  geom_boxplot(fill = "#EDD971", alpha = 0.1, outlier.shape = NA) +    
  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.8, size = 1) + 
  scale_colour_casa() +  # applies the default casaviz discrete palette
  labs(
    title = "Attainment 8 - All Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = NULL,
    colour = "School Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(10, "mm"),   # increase dot space
    legend.text = element_text(size = 10)  # optional: larger legend labels
  ) + guides(colour = guide_legend(override.aes = list(size = 4)))
```

-   Notice anything now?
-   What could/should we do about it? Any suggestions?

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(ggplot2)
library(dplyr)

# Filter out unwanted categories
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

# Recalculate stats on the filtered data
median_value <- median(england_filtered$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_filtered$ATT8SCR, na.rm = TRUE)

ggplot(england_filtered, aes(x = ATT8SCR, y = "")) +
  geom_boxplot(fill = "#EDD971", alpha = 0.1, outlier.shape = NA) +    
  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.5, size = 1) + 
  # Median + mean lines
  # Custom fixed palette matching previous graph
  scale_colour_manual(
    values = c(
      "Academy"          = "#2E6260",
      "Maintained school" = "#E16FCA"
    )
  ) +
  labs(
    title = "Attainment 8 - Academy and Maintained Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = NULL,
    colour = "School Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(4, "mm")
  ) +
  guides(colour = guide_legend(override.aes = list(size = 3)))
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram2
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)

# Filter data
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

# Recalculate stats for filtered data
median_value <- median(england_filtered$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_filtered$ATT8SCR, na.rm = TRUE)
sd_value   <- sd(england_filtered$ATT8SCR, na.rm = TRUE)

ggplot(england_filtered, aes(x = ATT8SCR)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Attainment 8 - Academy and Maintained Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = "Density"
  ) +
  theme_minimal()
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram3
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)

# Filter data
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

# Recalculate stats for filtered data
median_value <- median(england_filtered$PTFSM6CLA1A, na.rm = TRUE)
mean_value   <- mean(england_filtered$PTFSM6CLA1A, na.rm = TRUE)
sd_value   <- sd(england_filtered$PTFSM6CLA1A, na.rm = TRUE)

ggplot(england_filtered, aes(x = PTFSM6CLA1A)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "% Disadvantaged Students - Academy and Maintained Schools 2022/23 Academic Year",
    x = "% Disadvantaged",
    y = "Density"
  ) +
  theme_minimal()
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

-   The process of exploring and interrogating your data should take much longer than the modelling at the end
-   The process is iterative - you might well be exploring your data and the theory ***at the same time*** to help you explore, filter, select and prepare for the modelling phase
-   Understanding your 'system' and what the key elements are is vital.
    -   Sometimes this might come from experience (e.g. I used to be a school teacher so know that 'Special Schools' teach children with complex needs where academic performance is likely to be lower or that if you are at 'college' \[usually post-16\], then likely to be re-taking or studying for a reduced programme of GCSEs))
    -   more often you will have to carry out your own research to help you understand your system and its key elements

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

-   The role of theory and wider research on your system is vital in helping you select your variables for investigation
-   **DO NOT JUST THROW EVERYTHING INTO YOUR MODEL JUST BECAUSE YOU HAVE SOME VARIABLES**
    -   This is a common mistake
    -   It can also lead to spurious interpretation where correlation and causation are not the same thing
-   Carrying out a thorough literature review will also give you context for interpreting your model results later on in the process

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   Brighton and Hove City Council relied heavily on the work of Gorard in building its policy
-   Paper links social mixing to improved attainment for disadvantaged pupils
-   Includes variables such as:
    -   school type (e.g. academy, maintained)
    -   pupil characteristics (e.g. free school meals (FSM) eligibility, special educational needs, ethnicity)
    -   school characteristics (e.g. size, location)
-   Thus all might be worth investigating
:::

::: {.column width="40%"}
![](images/gorard_disad_paper.png)
:::
:::::

::: footer
<https://journals.sagepub.com/doi/full/10.1177/2158244018825171>
:::

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   However, wider reading also suggests that factors such as attendance (which Gordard does not include in his paper as a variable) may also play a big role in the attainment of disadvantaged pupils
-   Work by Claymore suggests that a large proportion of the gap in attainment between disadvantaged pupils and more affluent peers can be explained by:
    -   the differences in absence rates
    -   exclusion
    -   rates of moving between schools
:::

::: {.column width="40%"}
![](images/nfer.png)
:::
:::::

::: footer
<https://www.nfer.ac.uk/publications/being-present-the-power-of-attendance-and-stability-for-disadvantaged-pupils/>
:::

## Step 2 - Method {transition="convex-in none-out" transition-speed="fast"}

![](images/stat_tests.png){width="80%"}

::: Footer
<https://statsandr.com/blog/files/overview-statistical-tests-statsandr.pdf>
:::

## Step 2 - Method {transition="convex-in none-out" transition-speed="fast"}

-   ***Based on our raw ingredients, which methods might allow us to try and answer our research question(s)?***
-   ***Dependent variable(s)*** (what we are trying to explain / predict): Attainment 8 / Progress 8 **-\>** continuous / Ratio scales
-   ***Independent variables*** - explanatory / predictor variables of varying data types to explain variation in the dependent variable
-   As such ***Linear Regression*** is the most appropriate statistical test to employ

## Linear Regression - It's just a scatter plot!

```{r}
#| warning: FALSE
#| out-width: "60%"
#| fig-align: "center"
## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

#| warning: FALSE
#| out-width: "60%"
#| fig-align: "center"

## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Define custom color palette
custom_colors <- c(
  "#2E6260", "#658E62", "#9DBB65", "#C4CE6A", "#E7D870",
  "#F3C486", "#E993AC", "#D069BD", "#8F5289", "#4E3C56"
)

school_levels <- unique(btn_sub$SCHNAME.x)
school_colors <- setNames(
  rep(custom_colors, length.out = length(school_levels)),
  school_levels
)


# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)

# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

# Assign colors to schools
school_levels <- unique(btn_sub$SCHNAME.x)
school_colors <- setNames(
  rep(custom_colors, length.out = length(school_levels)),
  school_levels
)

# Plot with residual lines and custom colors
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR, color = SCHNAME.x)) +
  geom_point(size = 3, alpha = 0.9) +
  labs(
    title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score",
    color = "School"
  ) +
  scale_color_manual(values = school_colors) +
  theme_minimal() +
  theme(
    legend.position = c(1, 1),           # top-right inside plot
    legend.justification = c(1, 1)
  )
```

-   **A regression model is nothing more than a description of a scatter plot**
-   Dependent variable = $Y$-axis
-   Independent variable = $X$-axis

## Linear Regression - Line of Best-fit

```{r}
#| out-width: "60%"
#| fig-align: "center"
#| 
## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

# Plot with annotation
ggplot(btn_sub) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The linear regression model is the straight line of best-fit
-   A linear model function `lm()` (in `R`) uses a method called ***Ordinary Least Squares (OLS)*** to find the line of best-fit.
-   The best line minimises the squared (so that negatives and positives don't candel) vertical distances between the line and the points - hence OLS

## Linear Regression - Residuals / Error

```{r}
#| out-width: "60%"
#| fig-align: "center"

# Filter data (assuming england_filtered is in your environment)
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)

# Get the R-squared value from the model summary
r2_value <- summary(lm_fit)$r.squared

# Create the annotation text with the R-squared value, rounded to 2 decimal places
annotation_text <- paste0("R² = ", round(r2_value, 2))

# Plot with residual lines and the new annotation
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predict(lm_fit)),
               linetype = "dotted", color = "grey50") +
  # Use the new R-squared annotation
  annotate("text", 
           x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, 
           hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The vertical distances between the points and the line of best fit are called the **residuals** - sometimes also referred to as the **errors** or $\epsilon$
-   The closer the points are to the line, the better the fit of the model

## Linear Regression - R-Squared

```{r}
#| out-width: "60%"
#| fig-align: "center"

# Filter data (assuming england_filtered is in your environment)
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)

# Get the R-squared value from the model summary
r2_value <- summary(lm_fit)$r.squared

# Create the annotation text with the R-squared value, rounded to 2 decimal places
annotation_text <- paste0("R² = ", round(r2_value, 2))

# Plot with residual lines and the new annotation
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predict(lm_fit)),
               linetype = "dotted", color = "grey50") +
  # Use the new R-squared annotation
  annotate("text", 
           x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, 
           hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The fit of the model is represented by the $R^2$ value and is calculated from the residuals. It is sometimes referred to as the ***coefficient of determination***
-   It describes how much of the variation in $Y$ (Attainment 8 score) is explained by the variation in $X$ (% Disadvantaged Students) - here 69%
-   The closer to 1, the better the fit of the model

## Linear Regression - R-Squared

![](images/Correlation_examples2.png)

-   It's easy to visually estimate your $R^2$ value from looking at how well correlated the points are. *NB as squared, always +*

::: footer
Source: <https://work.thaslwanter.at/Stats/html/statsRelation.html>
:::

## Linear Regression - Slope and Intercept

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)


# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()

```

-   The regression line itself can be described by an equation with two parameters / coefficients:
    -   The **intercept** - $\beta_0$ - which is the value of $Y$ when $X = 0$
    -   The **slope** - $\beta_1$ - which the change in the value of $Y$ for a 1 unit change in $X$

## Linear Regression - Model Estimates

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A, PERCTOT, OFSTEDRATING, gor_name)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)


# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  geom_point(aes(x = 35, y = 40.3), color = "#e16fca", size = 4) +  # Red dot added here
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()


```

$$Y = \beta_0 + \beta_1X_1 + \epsilon$$ $$\hat{Y} = 62.35 + (-0.63 \times X) + \epsilon$$ [$$40.3 = 62.35 + (-0.63 \times 35) + 0$$]{style="color:#e16fca"}

## Linear Regression - The Statistical Model

```{r}
#| out-width: "40%"
#| out-height: "50%"
#| fig-align: "center"
summary(lm_fit)
```

-   Scatter plots are an excellent intuitive way to understand the relationship between two continuous variables
-   Statistical software describes key plot features + other useful info

## Linear Regression - Running the Model

![](images/the_Call.png)

-   This is the code representation of the model equation we saw earlier:
    -   `lm()` is the function that fits a linear model
    -   `ATT8SCR` (Attainment 8 Score) is the dependent variable $Y$
    -   `~` means "is modelled by"
    -   `PTFSM6CLA1A` (% Disadvantaged Students) is the independent variable $X$
    -   `data = bnt_sub` is the dataset we are using which contains the variables

## Linear Regression - Residuals and Error

![](images/the_resids.png) ![](images/the_error.png)

-   The residual errors range from -8.3 (Longhill) to + 4.7 (Varndean)
-   The residual standard error of 4.087 = predictions for School's Attainment 8 score are off by about 4.1 points
-   Residual Standard Error (RSE) and $R^2$ are inversely related - A smaller RSE and larger $R^2$ both mean a more precise model
-   The F-Statistic is a ratio of the amount of variance in $Y$ explained by the model to that not. x 17.7 more - statistically significant <0.005

## Linear Regression - Degrees of Freedom

![](images/the_error.png)

-   DF are the ***Degrees of Freedom*** in the model. DF ***very*** important for understanding how reliable your $R^2$ value might be
    -   The first number (1) relates to the number of variables
    -   The second number (8) relates to the number of observations (cases) in the dataset, minus the number of parameters
    -   In our example we have 10 observations and 2 parameters (intercept and slope) so $10 - 2 = 8$ degrees of freedom

## Linear Regression - Degrees of Freedom

-   **General rule**: ***more degrees of freedom = more reliable model***
-   A model with many parameters vs observations will bend to fit those observations and not be a good generalisation - this is called ***overfitting***
-   A model with little freedom might *appear* to have a high $R^2$
    -   but it is likely to perform poorly on new, unseen data
    -   it has captured the characteristics of your specific dataset rather than an underlying truth
-   **General Rule**: ***the more observations in your dataset the better!***

## Linear Regression - Degrees of Freedom

-   **How many Degrees of Freedom are required for a reliable model?**
-   No single, universally agreed-upon number - aim to maintain a healthy ratio of observations to the number of parameters
-   A common rule of thumb is the 10:1 ratio - **10 observations for every 1 parameter** - although this is a rough guide. If in doubt, even up to 20:1 to be safe
-   In our example, we have 10 observations and 2 parameters = 5:1 ratio = ***so a potentially unreliable model - proceed with extreme caution!***

## Linear Regression - Variance

-   Degress of Freedom are also used to calculate the ***variance*** of the model
-   If you ran the same model on different samples of the same population - e.g instead of Brighton, you ran this model on schools in Leeds or Bristol or Liverpool - you would get different values for your coefficients each time
-   How similar these are to each other and more importantly the the whole population (all schools in England and Wales) is the ***variance*** - you want to minimise the variance in your coefficients if you want to generalise your model to the wider population
-   **We will return to the idea of variance later**

## Linear Regression - Coefficients 1

![](images/the_coefs.png)

-   The Estimates ($\beta$)
    -   Intercept - estimated Attainment 8 value when % Disadvantaged Children in a school is **zero**
    -   PTFSM6CLA1A (slope 1) - effect of a one-unit change in the % Disadvantaged Children on Attainment 8 - therefore linked to the units of the independent variable and not directly comparable across different variables if measured on different scales

## Linear Regression - Coefficients 2

![](images/the_coefs.png)

-   The Standard Error (SE) - measure of the uncertainty or precision of the estimate
    -   Large SE relative to the Estimate = estimate unreliable / uncertain
    -   SE of 0.15 for the estimate of -0.63 for PTFSM6CLA1A means the change in the value of Attainment 8 for a 1% change in disadvantaged students could vary between -0.48 and -0.78

## Linear Regression - Coefficients 3

![](images/the_coefs.png)

-   The t-value - ratio of the Estimate to the SE
    -   a large standard error will make the t-value small (close to zero)
    -   ***t-values can be thought of as a standardised coefficient*** - very useful for comparing the relative importance of different predictors in the model
    -   The larger the t-value, the more significant the predictor is in explaining the variation in the dependent variable (more importance in your model)

## Linear Regression - Coefficients 4

![](images/the_coefs.png)

-   The p-value - **p**robability of observing a t-value as extreme as the one calculated if there were ***no relationship*** between the independent and the dependent variable
-   A small p-value (typically \< 0.05) indicates <5% change that $X$ not explaining variation in $Y$
-   The Signif. codes (like \*\*\* and \*\*) are just a quick visual guide to this p-value, showing you at a glance which variables are significant.
-   Any parameter with 1, 2 or 3\* is "statistically significant" at 5% or better

## Linear Regression - Coefficients 4

![](images/the_coefs.png)

-   The p-values relate to the ***null hypotheses*** that:
    1.   true value of the intercept is **zero** when the independent variable is **zero**. P \<0.001 = \<0.1% probability that 62.3 occured by random chance
        -   It doesn't tell us that when % Disadvantaged is zero, 62.3 will be a correct Attainment 8 score, just real value is unlikely to be zero
    2.   true value of the PTFSM6CLA1A slope (relationship between % Disadvantaged and Attainment 8) is zero. P \< 0.01 = \<1% probability that the relationship observed is by chance. Relationship is likely to be real.

## Linear Regression - Summary so far

::::: columns
::: {.column width="60%"}
1.  Variation in % disadvantaged students in schools in Brighton appears to explain about 65-68% of the variation in Attainment 8 at the school level

2.  The % disadvantaged students is a statistically significant predictor and the relationship appears to be linear

3.  A 1% reduction in the number of disadvantaged students in a school appears to be associated with a 0.62 point increase in Attainment 8, and vice versa

    **So the council was right? Well, not quite...**
:::

::: {.column width="40%"}
```{r}
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()
```
:::
:::::

## Linear Regression - Summary so far

::::: columns
::: {.column width="60%"}
4.  The degrees of freedom in the model are small, so model likely to be overfitted and unreliable for generalisation

-   Small numbers of observations (schools in the city) mean that the model is highly sensitive to changes in the data
-   It's also unclear whether there are any other factors that might be correlated with disadvantaged students that might also be influencing Attainment 8 scores and ***confounding (return to this later)*** the apparent relationship we observe
-   ***What are the practical consequences of overfitting?***
:::

::: {.column width="40%"}
```{r}
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()
```
:::
:::::

## Linear Regression - Overfitting, Outliers and High Leverage Points

-   When we overfit, the model can be overly influenced by ***outliers*** (large residuals) and changes in key ***leverage points*** (points that are far from the mean of the independent variable)
-   What happens to our model if a school improvement plan brings the outlier (Longhill) closer to attainment at Hove Park?
-   Council makes Varndean increase its disadvantaged intake to 30%?
-   And BACA improves its Attainment 8 to the city average following a huge cash injection from its academy trust?

## Linear Regression - Overfitting, Outliers and High Leverage Points

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 40%

btn_edit <- btn_sub %>%
  mutate(ATT8SCR = ifelse(SCHNAME.x == "Longhill High School", 46, ATT8SCR),
         ATT8SCR = ifelse(SCHNAME.x == "Brighton Aldridge Community Academy", 46, ATT8SCR),
         PTFSM6CLA1A = ifelse(SCHNAME.x == "Varndean School", 30, PTFSM6CLA1A))

# Fit linear model and get predicted values
lm_edit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_edit)
btn_edit <- btn_edit %>%
  mutate(predicted = predict(lm_edit),
         residual = ATT8SCR - predicted)

annotation_edit <- paste0("y = ", round(coef(lm_edit)[1], 2), " + ", round(coef(lm_edit)[2], 2), "x")

ggplot(btn_edit, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_edit)[1], slope = coef(lm_edit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_edit$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_edit$ATT8SCR, na.rm = TRUE), 
           label = annotation_edit, hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()
```

-   Plausible changes to three schools have almost completely removed relationship between % Disadvantaged Students & Attainment 8
-   Overfitting -\> parameters highly sensitive to minor changes to a few key data points
-   The ***closer the best-fit line gets to horizontal***, the closer we get to **NO RELATIONSHIP** between the independent and dependent variables

## Linear Regression - Overfitting, Outliers and High Leverage Points

```{r}
summary(lm_edit)
```

-   Overall p-value of model now **statistically insignificant** \>0.1
-   \% Disadvantage in Schools now **statistically insignificant**

## Linear Regression - More Degrees of Freedom

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Fit model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), alpha = 0.6) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 2, vjust = -7, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   Now we have a model that includes all schools in England and Wales
-   Negative association present - appears to confirm Gorard's observation
-   R-Squared is reasonable - 33% of the variation explained

## Linear Regression - More Degrees of Freedom

```{r}
summary(lm_fit)

```

-   We now have 3248 degrees of freedom
-   Intercept and % Disadvantaged Students both highly significant
-   Life is good? ***Wrong... Houston, we have some more problems!!***

## Linear Regression - Other Regression Assumptions

-   I forgot to mention - there are another set of rules to follow to ensure our linear regression is reliable:
    -   **Linearity**: The relationship between the independent and dependent variables is linear
    -   **Homoscedasticity**: Constant variance of residuals across all levels of the independent variable
    -   **Normality of residuals**: Residuals are normally distributed
    -   **No multicollinearity**: Independent variables are not highly correlated
    -   **Independence of residuals**: Errors are not related to each other with each other

## Linear Regression - Linearity?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Fit model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), alpha = 0.6) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 2, vjust = -7, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   Linearity - Does your line go through all the points nicely?
-   Hmmmm - suggestion of non-linearity with big up-tick at the low end of disadvantage

## Linear Regression - Linearity?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

-   Ploting the residuals against the fitted values is one check for linearity
-   The residuals should be randomly scattered around zero - if they are not, it suggests a non-linear relationship
-   The reference line should be horizontal at zero

## Linear Regression - Homoscedasticity (Constant variance)?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("homogeneity"))
```

-   Issues here too: the residuals are not randomly scattered around zero

## Linear Regression - Normality of Residuals?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

-   This is known as a **Q-Q plot** (Quantile-Quantile plot)
-   A number of points not on the line - suggesting the residuals are not normally distributed
-   Many points off line could mean untrustworthy p-values

## Linear Regression - Outliers?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("outliers"))
```

-   At least we don't have any problems with outliers!

## Linear Regression - Diagnostics

-   **Linearity**: Failed
-   **Homoscedasticity**: Failed
-   **Normality of residuals**: Failed
-   **No multicollinearity**: Something for later
-   **Independence of residuals**: Something for later
-   Presently our model violates most of the key assumptions that underpin linear regression models. What this means is that in its current form, the relationship between % Disadvantaged Students and Attainment 8 described by the model is not reliable

## Linear Regression - Non-Linearity
$$log(Y) = \beta_0 + \beta_1log(X_1) + \epsilon$$
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 50%

england_filtered <- england_filtered %>%
  filter(PTFSM6CLA1A > 0, ATT8SCR > 0)
england_filtered <- england_filtered %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

btn_sub <- btn_sub %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

# Fit model
lm_fit <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = log_ptfsm, y = log_att8, alpha = 0.6)) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = log_ptfsm, y = log_att8), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = log_ptfsm, y = log_att8), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$log_ptfsm, na.rm = TRUE), 
           y = min(england_filtered$log_att8, na.rm = TRUE), 
           label = annotation_text, hjust = 3, vjust = -2, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "log(% Disadvantaged Students)",
       y = "log(Average Attainment 8 Score)") +
  theme_minimal()
```

-   The log($Y$) log($X$) model is a standard transformation of these variables that can help to linearise relationships
-   Sometimes referred to as an *elasticity* model, a % (rather than constant) change in $X$ leads to a % (not constant) change in $Y$

## Linear Regression - the log-log transformation

```{r}
#| label: attainment-8-histogram4
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)


# Calculate log-transformed summary stats
mean_value <- mean(log(england_filtered$ATT8SCR), na.rm = TRUE)
median_value <- median(log(england_filtered$ATT8SCR), na.rm = TRUE)
sd_value <- sd(log(england_filtered$ATT8SCR), na.rm = TRUE)

ggplot(england_filtered, aes(x = log(ATT8SCR))) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 2)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 2)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Log-Transformed Attainment 8 - Academy and Maintained Schools 2022/23",
    x = "Log(Attainment 8 Score)",
    y = "Density"
  ) +
  theme_minimal()
```

## Linear Regression - the log-log transformation

```{r}
mean_ptfsm <- mean(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)
median_ptfsm <- median(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)
sd_ptfsm <- sd(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)

ggplot(england_filtered, aes(x = log(PTFSM6CLA1A))) +
  geom_histogram(aes(y = ..density..), binwidth = 0.15, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_ptfsm, sd = sd_ptfsm),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_ptfsm, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_ptfsm, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_ptfsm, y = Inf,
           label = paste0("Median = ", round(median_ptfsm, 2)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_ptfsm, y = Inf,
           label = paste0("Mean = ", round(mean_ptfsm, 2)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Log-Transformed % Disadvantaged Students - Academy and Maintained Schools 2022/23",
    x = "Log(% Disadvantaged Students)",
    y = "Density"
  ) +
  theme_minimal()
```



## Linear Regression - the log-log transformation

```{r}
summary(lm_fit)
```

-   Everything now looks highly statistically significant
-   $R^2$ is much better than before - 45%

## Linear Regression - the log-log transformation

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

-   The residuals are now randomly scattered around zero - suggesting a linear relationship

## Linear Regression - the log-log transformation

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

-   The residuals are now normally distributed - the Q-Q plot shows most points on the line

## Linear Regression - the log-log transformation

-   Interpreting the log-log relationship:
    -   Where the relationship is negative, a small absolute change in the independent variable at the *lower* end has a *bigger* impact than the same small absolute change at the higher end
    -   On our original plot: the effect of levels of disadvantage on attainment is much ***stronger at very low levels of disadvantage*** and ***weakens as the level of disadvantage increases***

## Linear Regression - Level-log model
$$Y = \beta_0 + \beta_1log(X_1) + \epsilon$$
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 40%

england_filtered <- england_filtered %>%
  filter(PTFSM6CLA1A > 0, ATT8SCR > 0)
england_filtered <- england_filtered %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

btn_sub <- btn_sub %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

# Fit model
lm_fit <- lm(ATT8SCR ~ log(PTFSM6CLA1A), data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = log_ptfsm, y = ATT8SCR, alpha = 0.6)) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = log_ptfsm, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = log_ptfsm, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$log_ptfsm, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 4, vjust = -2, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "log(% Disadvantaged Students)",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```
-   Other options are to use a ***level-log*** model, where the dependent variable is not transformed but the independent variable is log-transformed.
-   Use when the effect of the independent variable has diminishing returns - e.g. a change from log(2.7%) exp(1) to log(7.3%) exp(2) disadvantaged students has same effect as a change from log(20%) exp(3) to log(54.5%) exp(4)

## Linear Regression - Level-log model

```{r}
summary(lm_fit)
```

## Linear Regression - Level-log model

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

## Linear Regression - Level-log model

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

## Linear Regression - to log or not to log?

-   The log-log model appears a better fit than the level-log model, however both are better than the basic linear model
-   How to proceed will depend a little on whether we want to extend our model with other explanatory variables and the relationship they have with attainment
-   Log-log - use when the relationship is believed to be multiplicative and the effects are best understood in terms of percentage changes. 
    -   This model is also excellent at addressing issues with heteroscedasticity and skewed variables
-   Level-log - use when the effect of the independent variable has diminishing returns

## Linear Regression - extending the model

-   Mathematically (if not visually) we can add as many additional dependent variables as we like to increase the explanatory power of our model

$$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \dots + \beta_kX_{ki} + \epsilon_i$$

where:

-   $Y_i$ is the dependent variable of observation $i$
-   $X_{1i}, X_{2i}, ..., X_{ki}$ are the independent variables for observation $i$
-   $\beta_0$ is the intercept (constant term)
-   $\beta_1, \beta_2, ..., \beta_k$ are the coefficients for each independent variable
-   $\epsilon_i$ is the error term (residual) for observation $i$

## Linear Regression - extending the model

-   Sometimes in the literature you will see a shorthand matrix version of this equation, written:

$$Y = X{\beta} + {\epsilon}$$

where:

-   $Y$: A column vector of all the dependent variable observations.
-   $X$: A matrix called the "design matrix," which contains a column of 1s (for the intercept) and columns for the values of all your independent variables for every observation.
-   $\beta$: A column vector of all the coefficients you are estimating.
-   $\epsilon$: A column vector of all the residuals.

## Multiple Linear Regression

-   Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and multiple independent variables
-   As with bivariate regression, variables should be chosen based on theory and prior research rather than just throwing the variables in
-   However, it's permissible to experiment and iterate between data exploration and theory - both can inform each other

## Multiple Linear Regression

```{r}
# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = log(PERCTOT), y = ATT8SCR, alpha = 0.6)) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = log(PERCTOT), y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = log(PERCTOT), y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(log(england_filtered$PERCTOT), na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 2.5, vjust = -8, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 \nvs % Persistent Absence, 2022-23",
       x = "log(% Persistent Absence)",
       y = "log(Average Attainment 8 Score)") +
  theme_minimal()
```
-   If we recall from some of the background slides, significant research to suggest persistent absence an important factor in attainment

## Multiple Linear Regression

```{r}
# Fit linear model and get predicted values
lm_fit1 <- lm(log(ATT8SCR) ~ log(PERCTOT) , data = england_filtered)

summary(lm_fit1)
```

-   The simple bivariate model for persistent absence improves the $R^2$ value to 60% (compared to 45% for the % disadvantage model)
-   This suggests that persistent absence might explain more of the variation in Attainment 8, but one way to really tell is to put them in a model together

## Multiple Linear Regression

```{r}
#| out-width: 80%
#| align: center

library(plotly)

fig <- plot_ly(england_filtered, x = ~log(PTFSM6CLA1A), y = ~log(ATT8SCR), z = ~log(PERCTOT), color = ~gor_name)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'log(% Disadvantaged)'),
                     yaxis = list(title = 'log(Attainment 8)'),
                     zaxis = list(title = 'log(% Persistent Absence)')))

fig
```

## Multiple Linear Regression

```{r}
# Fit linear model and get predicted values
lm_fit1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT), data = england_filtered)

summary(lm_fit1)
```
-   What is the model telling us?

## Multiple Linear Regression

-   The coefficients for both variables are statistically ***significant and negative***, indicating that both variables contribute to explaining the variation in Attainment 8 scores
-   But t-values indicate that the ***% Persistent Absence variable (-50.49) has a stronger effect on Attainment 8 scores than the % Disadvantaged Students variable (-31.16)***
-   The $R^2$ value is now 0.69, indicating that the model is potentially good and explains ***69% of the variation in Attainment 8 scores***. Degrees of freedom are good and the overall model is statistically significant.
-   But does is satisfy the assumptions of linear regression?

## Multiple Linear Regression - Diagnostics 1

```{r}

check_model(lm_fit1, 
            check = c("linearity"))

```
-   Pretty good - the residuals are randomly scattered around zero, suggesting a linear relationship

## Multiple Linear Regression - Diagnostics 2
```{r}
check_model(lm_fit1, 
            check = c("qq"))
```
-   The residuals are normally distributed - the Q-Q plot shows most points on the line

## Multiple Linear Regression - Diagnostics 3
```{r}
check_model(lm_fit1, 
            check = c("homogeneity"))
```
-   The residuals are randomly scattered around zero, suggesting constant variance.

## Linear Regression - Diagnostics

-   **Linearity**: Passed
-   **Homoscedasticity**: Passed
-   **Normality of residuals**: Passed
-   **No multicollinearity**: What is this?
-   **Independence of residuals**: What is this?

## Multiple Linear Regression - Multicollinearity

-   Multicollinearity occurs when two or more independent variables in a regression model are ***highly correlated*** with each other
-   This can lead to unreliable estimates of the coefficients, inflated standard errors, and difficulty in interpreting the results

## Multiple Linear Regression - Multicollinearity

```{r}
#| out-width: 70%
#| align: center
library(correlation)
library(see)

corr <- england_filtered |>
  correlation(select = c("PTFSM6CLA1A", "PERCTOT", "ATT8SCR"))

corr %>%
  summary(redundant = TRUE) %>%
  plot()

```

-   A quick and easy way to check for the correlation between your independent variables is generate a standard correlation matrix/plot
-   Difficult to say what is too much correlation - but over 0.7 is often considered problematic
-   However, pairwise correlations might miss n-way correlations

```{r}
check_model(lm_fit1, 
            check = c("vif"))
```

-   A more useful diagnostic tool is the ***Variance Inflation Factor (VIF)***, which measures how much the variance of a regression coefficient is increased due to multicollinearity
-   A VIF value of 1 indicates no correlation, while a VIF value above 5 or 10 suggests high multicollinearity

## Multiple Linear Regression - VIF

***Why does it even matter if the variance is inflated?***

-   ***Variance = Uncertainty*** in our model
-   If 2 or more variables are highly correlated and both appear to affect the dependent variable, it can be difficult to determine which variable is actually having the effect
-   The model makes an arbitrary split
-   Small changes in the data could make the split go either way - e.g. attribute more of the effect to % Disadvantaged Students or % Persistent Absence (if VIF high) - thus ***inflating*** the uncertainty / ***variance***

## Multiple Linear Regression - VIF

-   Despite some positive correlation (0.48) between % Disadvantaged Studets and % Persistent Absence, the VIF is very low ~1.5
-   No problems with multicollinearity
-   So we have passed that test and can happily use both in the model

## Linear Regression - Diagnostics

-   **Linearity**: Passed
-   **Homoscedasticity**: Passed
-   **Normality of residuals**: Passed
-   **No multicollinearity**: Passed
-   **Independence of residuals**: What is this?

## Multiple Linear Regression - Independence of Residuals

-   Independence of residuals means that the residuals (errors) of the model are not correlated with each other
-   This is important because if the residuals are correlated, it suggests that there is some pattern in the errors that the model has not captured, which can lead to biased estimates and incorrect conclusions
-   Correlated residuals can occur when there is a time or spatial component to the data, or when there are unobserved variables that are influencing the dependent variable

## Multiple Linear Regression - Spatial Or Temporal (auto)Correlation

-   In our case, we have a spatial component to the data - schools are located in different areas of England and Wales
-   If schools in the same area have similar characteristics, it is likely that the residuals will be correlated
-   The easiest way to check this is to plot the values of the residuals for the schools on a map

## Multiple Linear Regression - Spatial Or Temporal (auto)Correlation




## Multiple Linear Regression - Dummy Variables




## Multiple Linear Regression - Confounding


## Multiple Linear Regression - Interaction Effects




```{r}
# Fit linear model and get predicted values
lm_fit1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) , data = england_filtered)

summary(lm_fit1)
```


